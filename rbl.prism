dtmc






const reward_unseen_speed_1;
const reward_unseen_speed_2;
const reward_unseen_speed_3;
const reward_unseen_speed_4;
const reward_unseen_speed_5;
const total_resets;
const reward_reset_step;
const drn_reward_wait_threshold;
const HC_plasticity_threshold;
const pf_length; //length of traversable area in a placefield
const delay; //number of steps between agent entering a placefield and reward spawning
//random speeds for traversing placefield before OFC-DRN connection established
const mpfc_random_speed_lo; 
const mpfc_random_speed_med;
const mpfc_random_speed_hi;
const reward_seen_speed = 40; //speed after reward seen (vis2 = 1)
const ave_reward_pos_in_pf; //must be significantly less than pf_length
const reward_spread;

global reward_collected : bool init false;
global waiting : bool init false; // agent has slowed in front of reard location
global in_reward_pf : bool init false;





module agentMotor

//am_state=0: random movement to pf
//am_state=1: agent is in pf; if drn > drn_move_threshold AND pf corresponds with learned reward location, goto am_state = 3, else, goto am_state=2
//am_state=2: agent is advancing at random speed. If agent reaches reward landmark, goto am_state = todo (end)
//am_state=3: agent is advancing using speed determined by DRN output,If agent reaches reward landmark, goto am_state = todo (end)
//am_state=4: check if agent has received reward. output OFCtoDRN as necessary
//am_state=5: signal OFC 
//am_state=6: reset to am_state=0;

pos : [0..100] init 0;//0 is starting position //TODO rename pf
inpf_pos : [0..pf_length];
ss : [0..200] init 0; //reward location reset counter
am_state : [0..10] init 0; //state counter for this module
reward_pos_in_pf : [0..pf_length];

resets : [0..total_resets + 100] init 0;

learned : bool init false;


last_learned : [0..100] init 0;

vis1 : [0..1] init 0; // reward landmark seen
vis2 : [0..1] init 0; // reward seen

reward_pos : [0..100] init 1; //placefield containing reward




[] (am_state = 0 )-> //move randomly to a placefield  
1/5 : (pos'=1) & (am_state' = 1)  + 1/5 : (pos'=2) & (am_state' = 1)  + 1/5 : (pos'=3) &  (am_state' = 1) + 1/5 : (pos'=4) & (am_state'=1) + 1/5 : (pos'=5) & (am_state'=1);

[]am_state=1 & (pos<=3)->
	1/5 : (reward_pos_in_pf'=ave_reward_pos_in_pf) +
	1/5 : (reward_pos_in_pf'=ave_reward_pos_in_pf + reward_spread)  & (am_state'=2) +
	1/5 : (reward_pos_in_pf'=ave_reward_pos_in_pf + reward_spread*2)  & (am_state'=2)  +
	1/5 : (reward_pos_in_pf'=ave_reward_pos_in_pf - reward_spread)  & (am_state'=2)  +
	1/5 : (reward_pos_in_pf'=ave_reward_pos_in_pf - reward_spread*2)  & (am_state'=2) ;

[]am_state=1 & (pos>3)->(am_state'=6);

[] am_state=2 & (DRN>=drn_reward_wait_threshold)-> //if agent in same pf as learned reward location, goto am_state=4
	(am_state'=4) & (waiting'=true) & (vis1'=1) & (learned'=true);

[] am_state=2 & (DRN<drn_reward_wait_threshold) ->
 (am_state'=3)  & (waiting'=true) & (vis1'=1);


[agent_move] am_state=3 & inpf_pos + mpfc_random_speed_hi < pf_length -> //move forward using random speed 
	 1/3 : (inpf_pos' = inpf_pos + mpfc_random_speed_lo) +
	 1/3 : (inpf_pos' = inpf_pos + mpfc_random_speed_med) +
	 1/3 : (inpf_pos' = inpf_pos + mpfc_random_speed_hi); 



//agent advances each step by speed deterimed from DRN lookup table
[agent_move] am_state=4 & vis2=0 & inpf_pos + reward_unseen_speed_pf_1 < pf_length-> (inpf_pos'=inpf_pos+reward_unseen_speed_pf_1);



[agent_move] reward_present=true -> (vis2'=1); 

[agent_move] am_state=4 & vis2=1 & inpf_pos + reward_seen_speed < pf_length  -> (inpf_pos'=inpf_pos+reward_seen_speed);

[agent_move] (am_state=3 | am_state = 4) & inpf_pos>= reward_pos_in_pf ->
	(am_state'=5);

//updated

[] am_state=5 & pos = reward_pos & reward_present = true ->  //agent is in the placefield with the reward and the reward has appeared, so the agent collects the reward
	(reward_collected'=true) & (am_state'=6) & (waiting'=false) & (inpf_pos'=0);

[] am_state=5 & reward_present = true & pos != reward_pos -> //fake reward has appeared as agent is not in the correct placefield, so no reward gained.
	(reward_collected'=false) & (am_state'=6) & (waiting'=false) & (inpf_pos'=0);

[] am_state=5 & reward_present = false & pos = reward_pos ->//reward has not appeared and agent is in the correct placefield, so no reward gained
	(reward_collected'=false) & (am_state'=6) & (waiting'=false) & (inpf_pos'=0);

[] am_state=5 & reward_present = false & pos != reward_pos -> //reward has not appeared and agent is not in the correct placefield, so no reward gained
	(reward_collected'=false) & (am_state'=6) & (waiting'=false) & (inpf_pos'=0);

[signal_weight_change] (am_state = 6 & ss!=reward_reset_step) & ss + 1 < 100 ->(ss' = ss + 1) & (am_state' = 7); //send signal to OFC and reset am_state
//updated
[] (am_state = 6 & ss=reward_reset_step) & resets<total_resets+100 ->   //reset reward location on reset_reward_step
	1/3 :(pos'=0) & (am_state' = 7) & (reward_pos' = 1) & (ss' = 0) & (resets'=resets+1) + 
	1/3 :(pos'=0) & (am_state' = 7) & (reward_pos' = 2) & (ss' = 0) & (resets'=resets+1) + 
	1/3 :(pos'=0) & (am_state' = 7) & (reward_pos' = 3) & (ss' = 0) & (resets'=resets+1);
//updated
[] am_state = 7 & resets!=total_resets->(pos' = 0) & (am_state'=0) & (reward_collected'=false) & (in_reward_pf'=false) & (vis1'=0) & (vis2'=0);//reset variables for each run

[] am_state = 7 & resets=total_resets->(pos' = 0) & (am_state'=8) & (reward_collected'=false) & (in_reward_pf'=false) & (vis1'=0) & (vis2'=0);//reset variables for each run

[] am_state=8 -> (am_state'=8);
//updated

endmodule




module weight

unlearned : bool init false;

HC1toOFCkey : [0..100] init 0;
HC2toOFCkey : [0..100] init 0;
HC3toOFCkey : [0..100] init 0;


//increase HC to OFC plasticity
[signal_weight_change] DRN>=HC_plasticity_threshold & HC1=1 & HC1toOFCkey<2->  
	(HC1toOFCkey'=HC1toOFCkey+1);
[signal_weight_change] DRN>=HC_plasticity_threshold & HC2=1 & HC2toOFCkey<2->  
	(HC2toOFCkey'=HC2toOFCkey+1);
[signal_weight_change] DRN>=HC_plasticity_threshold & HC3=1 & HC3toOFCkey<2->  
	(HC3toOFCkey'=HC3toOFCkey+1);

//upper limit HC to OFC plasticity
[signal_weight_change] DRN>=HC_plasticity_threshold & HC1=1 & HC1toOFCkey>=2->  
	(HC1toOFCkey'=HC1toOFCkey);
[signal_weight_change] DRN>=HC_plasticity_threshold & HC2=1 & HC2toOFCkey>=2->  
	(HC2toOFCkey'=HC2toOFCkey);
[signal_weight_change] DRN>=HC_plasticity_threshold & HC3=1 & HC3toOFCkey>=2->  
	(HC3toOFCkey'=HC3toOFCkey);


//decrease HC to OFC plasticity
[signal_weight_change] DRN<HC_plasticity_threshold & HC1=1 & HC1toOFCkey>0->  
	(HC1toOFCkey'=HC1toOFCkey-1) & (unlearned'=true);
[signal_weight_change] DRN<HC_plasticity_threshold & HC2=1 & HC2toOFCkey>0->  
	(HC2toOFCkey'=HC2toOFCkey-1) & (unlearned'=true);
[signal_weight_change] DRN<HC_plasticity_threshold & HC3=1 & HC3toOFCkey>0->  
	(HC3toOFCkey'=HC3toOFCkey-1) & (unlearned'=true);




//lower limit HC to OFC plasticity
[signal_weight_change] DRN<HC_plasticity_threshold & HC1=1 & HC1toOFCkey=0->  
	(HC1toOFCkey'=HC1toOFCkey);
[signal_weight_change] DRN<HC_plasticity_threshold & HC2=1 & HC2toOFCkey=0->  
	(HC2toOFCkey'=HC2toOFCkey);
[signal_weight_change] DRN<HC_plasticity_threshold & HC3=1 & HC3toOFCkey=0->  
	(HC3toOFCkey'=HC3toOFCkey);

//do nothing

[signal_weight_change]DRN<HC_plasticity_threshold & HC1=0 & HC2=0 & HC3=0->
	 (HC1toOFCkey'=HC1toOFCkey) & (HC2toOFCkey'=HC2toOFCkey) & (HC3toOFCkey'=HC3toOFCkey);





endmodule





module reward_spawner


c : [0..delay] init 0; //reward spawn delay counter CANNOT be zero
t : [0..10] init 0;  //state counter to help ordering of events


despwnd : bool init false;
reward_present : bool init false;

//[] (t=0) & waiting=false -> 1: (t' = 0);
[] (t=0) & waiting=true -> 1: (t' = 1);

[agent_move] (t=1) & reward_present=false & c<delay-> 1: (c'=c+1);  //increment reward spawn counter
[agent_move] (t=1) & reward_present=false & c>=delay -> (reward_present'=true) & (c'=0) & (t'=2); //spawn reward after "delay" steps
[agent_move] waiting=true & (t=2) & reward_present=true -> (t'=2);
[] waiting=false -> (t'=0) & (c'=0) & (reward_present'=false);

 
endmodule


formula HC1 = (pos=1?1:0);
formula HC2 = (pos=2?1:0);
formula HC3 = (pos=3?1:0);


formula LH = (reward_collected=true?1:0);




//lookup tables for OFC to DRN

//lookup tables for HC to OFC
formula HC1toOFC = HC1toOFCkey=0?10:(HC1toOFCkey=1?30:(HC1toOFCkey=2?90:(HC1toOFCkey=2?200:0)));
formula HC2toOFC = HC2toOFCkey=0?10:(HC2toOFCkey=1?30:(HC2toOFCkey=2?90:(HC2toOFCkey=2?200:0)));
formula HC3toOFC = HC3toOFCkey=0?10:(HC3toOFCkey=1?30:(HC3toOFCkey=2?90:(HC3toOFCkey=2?200:0)));

formula x = vis1 + vis2; //speed based on visual inputs

formula OFC = HC1*HC1toOFC + HC2*HC2toOFC + HC3*HC3toOFC;




formula DRN = LH=1?(OFC=10?200:OFC=30?300:OFC=90?400:OFC=200?500:0):(OFC=10?10:OFC=30?40:OFC=90?90:OFC=200?200:0); //todo get 5 proper values

// reward_unseen_speed formula for each pf based on OFCxtoDRN connection strength



formula reward_unseen_speed_pf_1 = (DRN = 10?reward_unseen_speed_1:(DRN = 30?reward_unseen_speed_2:(DRN = 90?reward_unseen_speed_3:(DRN = 200?reward_unseen_speed_4:(DRN = 200?reward_unseen_speed_5:0)))));
formula reward_unseen_speed_pf_2 = (DRN = 10?40:(DRN = 30?20:(DRN = 90?10:(DRN = 200?5:0))));
formula reward_unseen_speed_pf_3 = (DRN = 10?40:(DRN = 30?20:(DRN = 90?10:(DRN = 200?5:0))));


//todo get all speed values





